{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, autograd as ag\n",
    "import numpy as np\n",
    "import pdb\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn.functional as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        self.sigma_weight = nn.Parameter(torch.Tensor(out_features, in_features).fill_(sigma_init))\n",
    "        self.register_buffer(\"epsilon_weight\", torch.zeros(out_features, in_features))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.Tensor(out_features).fill_(sigma_init))\n",
    "            self.register_buffer(\"epsilon_bias\", torch.zeros(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        nn.init.uniform(self.weight, -std, std)\n",
    "        nn.init.uniform(self.bias, -std, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        torch.randn(self.epsilon_weight.size(), out=self.epsilon_weight)\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            torch.randn(self.epsilon_bias.size(), out=self.epsilon_bias)\n",
    "            bias = bias + self.sigma_bias * Variable(self.epsilon_bias)\n",
    "        return F.linear(input, self.weight + self.sigma_weight * Variable(self.epsilon_weight), bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_mat = torch.tensor([[1., 0.2, 0.4],\n",
    "#                      [0.2, 1., 0.7],\n",
    "#                      [0.4, 0.7, 1.0]])\n",
    "# loc = torch.tensor([0.9, 0.5, 0.1])\n",
    "corr_mat = torch.tensor([[1.00, -0.85, -0.78,  0.68, -0.87,  0.42],\n",
    "[-0.85, 1.00,  0.79, -0.71,  0.89, -0.43],\n",
    "[-0.78,  0.79,  1.00, -0.45,  0.66, -0.71],\n",
    "[0.68, -0.71, -0.45,  1.00, -0.71,  0.09],\n",
    "[-0.87,  0.89,  0.66, -0.71,  1.00, -0.17],\n",
    "[0.42, -0.43, -0.71,  0.09, -0.17,  1.00]])\n",
    "loc = torch.tensor([18.1,  225, 105, 2.76, 3.460, 20.22])\n",
    "\n",
    "scaling = torch.distributions.Uniform(torch.tensor(0.), torch.tensor(1.))\n",
    "mvn = torch.distributions.MultivariateNormal(loc, corr_mat )\n",
    "data = torch.cat([mvn.sample() * scaling.sample() for i in range(100)]).view(100, -1)\n",
    "\n",
    "from sklearn import datasets\n",
    "data = torch.from_numpy(datasets.make_blobs(n_features=6, n_samples=100, centers=10, center_box=[0, 1000])[0]).view(100, -1).type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(vec):\n",
    "    return vec + torch.distributions.Normal(0, 0.5).sample_n(len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, w = None):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.embed = nn.Embedding(100, 32)\n",
    "        if w is not None:\n",
    "            self.embed.weight = nn.Parameter(w)\n",
    "        #self.fc1 = NoisyLinear(16, 16) \n",
    "        self.fc1 = nn.Linear(32, 16) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        #self.fc2 = NoisyLinear(16, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        #self.fc3 = NoisyLinear(16, 6)\n",
    "        self.fc3 = nn.Linear(16, 6)\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  \n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 14072.7188, Acc: 0.53%\n",
      "Epoch [2/200], Loss: 11022.4990, Acc: 0.53%\n",
      "Epoch [3/200], Loss: 9254.6064, Acc: 0.52%\n",
      "Epoch [4/200], Loss: 15682.7734, Acc: 0.45%\n",
      "Epoch [5/200], Loss: 5963.8955, Acc: 0.47%\n",
      "Epoch [6/200], Loss: 5970.9004, Acc: 0.5%\n",
      "Epoch [7/200], Loss: 5103.2007, Acc: 0.53%\n",
      "Epoch [8/200], Loss: 7536.0850, Acc: 0.53%\n",
      "Epoch [9/200], Loss: 8667.5508, Acc: 0.5%\n",
      "Epoch [10/200], Loss: 10445.3291, Acc: 0.53%\n",
      "Epoch [11/200], Loss: 12857.0322, Acc: 0.55%\n",
      "Epoch [12/200], Loss: 10103.3369, Acc: 0.56%\n",
      "Epoch [13/200], Loss: 11088.2373, Acc: 0.57%\n",
      "Epoch [14/200], Loss: 14248.4912, Acc: 0.56%\n",
      "Epoch [15/200], Loss: 8987.9688, Acc: 0.56%\n",
      "Epoch [16/200], Loss: 9018.5400, Acc: 0.56%\n",
      "Epoch [17/200], Loss: 11773.4121, Acc: 0.56%\n",
      "Epoch [18/200], Loss: 10582.8340, Acc: 0.56%\n",
      "Epoch [19/200], Loss: 11043.0117, Acc: 0.56%\n",
      "Epoch [20/200], Loss: 10980.7930, Acc: 0.55%\n",
      "Epoch [21/200], Loss: 10308.3047, Acc: 0.53%\n",
      "Epoch [22/200], Loss: 9843.3477, Acc: 0.53%\n",
      "Epoch [23/200], Loss: 14603.4570, Acc: 0.55%\n",
      "Epoch [24/200], Loss: 6500.9302, Acc: 0.55%\n",
      "Epoch [25/200], Loss: 5581.9121, Acc: 0.55%\n",
      "Epoch [26/200], Loss: 4686.5229, Acc: 0.55%\n",
      "Epoch [27/200], Loss: 6532.0283, Acc: 0.55%\n",
      "Epoch [28/200], Loss: 4744.2910, Acc: 0.55%\n",
      "Epoch [29/200], Loss: 14142.3525, Acc: 0.55%\n",
      "Epoch [30/200], Loss: 13770.0117, Acc: 0.55%\n",
      "Epoch [31/200], Loss: 12664.3096, Acc: 0.56%\n",
      "Epoch [32/200], Loss: 7743.4717, Acc: 0.55%\n",
      "Epoch [33/200], Loss: 9075.6846, Acc: 0.54%\n",
      "Epoch [34/200], Loss: 10747.3721, Acc: 0.56%\n",
      "Epoch [35/200], Loss: 7436.9893, Acc: 0.56%\n",
      "Epoch [36/200], Loss: 10859.8008, Acc: 0.56%\n",
      "Epoch [37/200], Loss: 12326.3525, Acc: 0.53%\n",
      "Epoch [38/200], Loss: 8856.6904, Acc: 0.54%\n",
      "Epoch [39/200], Loss: 6522.7246, Acc: 0.55%\n",
      "Epoch [40/200], Loss: 14394.2266, Acc: 0.56%\n",
      "Epoch [41/200], Loss: 10157.4854, Acc: 0.56%\n",
      "Epoch [42/200], Loss: 12473.5723, Acc: 0.56%\n",
      "Epoch [43/200], Loss: 8897.4688, Acc: 0.56%\n",
      "Epoch [44/200], Loss: 9195.8730, Acc: 0.58%\n",
      "Epoch [45/200], Loss: 8599.0039, Acc: 0.57%\n",
      "Epoch [46/200], Loss: 7144.2949, Acc: 0.57%\n",
      "Epoch [47/200], Loss: 6294.3975, Acc: 0.57%\n",
      "Epoch [48/200], Loss: 12076.7852, Acc: 0.57%\n",
      "Epoch [49/200], Loss: 8409.9141, Acc: 0.56%\n",
      "Epoch [50/200], Loss: 8797.2227, Acc: 0.56%\n",
      "Epoch [51/200], Loss: 11084.0654, Acc: 0.57%\n",
      "Epoch [52/200], Loss: 11565.9941, Acc: 0.56%\n",
      "Epoch [53/200], Loss: 14026.6904, Acc: 0.56%\n",
      "Epoch [54/200], Loss: 7550.6562, Acc: 0.56%\n",
      "Epoch [55/200], Loss: 8930.5947, Acc: 0.56%\n",
      "Epoch [56/200], Loss: 11043.1338, Acc: 0.56%\n",
      "Epoch [57/200], Loss: 6109.8198, Acc: 0.56%\n",
      "Epoch [58/200], Loss: 7066.3545, Acc: 0.56%\n",
      "Epoch [59/200], Loss: 4501.7808, Acc: 0.56%\n",
      "Epoch [60/200], Loss: 10969.9697, Acc: 0.56%\n",
      "Epoch [61/200], Loss: 5225.9614, Acc: 0.56%\n",
      "Epoch [62/200], Loss: 12944.8115, Acc: 0.56%\n",
      "Epoch [63/200], Loss: 7424.4624, Acc: 0.57%\n",
      "Epoch [64/200], Loss: 12653.9883, Acc: 0.56%\n",
      "Epoch [65/200], Loss: 6565.4507, Acc: 0.57%\n",
      "Epoch [66/200], Loss: 9115.2246, Acc: 0.57%\n",
      "Epoch [67/200], Loss: 9354.1875, Acc: 0.56%\n",
      "Epoch [68/200], Loss: 15030.1289, Acc: 0.56%\n",
      "Epoch [69/200], Loss: 11023.4424, Acc: 0.56%\n",
      "Epoch [70/200], Loss: 9246.4160, Acc: 0.56%\n",
      "Epoch [71/200], Loss: 9879.6865, Acc: 0.56%\n",
      "Epoch [72/200], Loss: 3789.6296, Acc: 0.57%\n",
      "Epoch [73/200], Loss: 8948.9971, Acc: 0.57%\n",
      "Epoch [74/200], Loss: 5723.9644, Acc: 0.57%\n",
      "Epoch [75/200], Loss: 4680.0957, Acc: 0.57%\n",
      "Epoch [76/200], Loss: 7156.5176, Acc: 0.56%\n",
      "Epoch [77/200], Loss: 7027.7671, Acc: 0.57%\n",
      "Epoch [78/200], Loss: 6942.1699, Acc: 0.57%\n",
      "Epoch [79/200], Loss: 4727.5337, Acc: 0.57%\n",
      "Epoch [80/200], Loss: 7739.5352, Acc: 0.57%\n",
      "Epoch [81/200], Loss: 11477.0098, Acc: 0.55%\n",
      "Epoch [82/200], Loss: 12152.4102, Acc: 0.57%\n",
      "Epoch [83/200], Loss: 13685.4736, Acc: 0.58%\n",
      "Epoch [84/200], Loss: 12970.2383, Acc: 0.58%\n",
      "Epoch [85/200], Loss: 14819.3975, Acc: 0.57%\n",
      "Epoch [86/200], Loss: 12098.3359, Acc: 0.57%\n",
      "Epoch [87/200], Loss: 7649.1836, Acc: 0.57%\n",
      "Epoch [88/200], Loss: 8604.8516, Acc: 0.57%\n",
      "Epoch [89/200], Loss: 9249.6436, Acc: 0.58%\n",
      "Epoch [90/200], Loss: 8347.5010, Acc: 0.57%\n",
      "Epoch [91/200], Loss: 5105.3140, Acc: 0.57%\n",
      "Epoch [92/200], Loss: 7246.5068, Acc: 0.57%\n",
      "Epoch [93/200], Loss: 7993.7817, Acc: 0.57%\n",
      "Epoch [94/200], Loss: 5379.3521, Acc: 0.57%\n",
      "Epoch [95/200], Loss: 4239.4414, Acc: 0.56%\n",
      "Epoch [96/200], Loss: 6023.3887, Acc: 0.57%\n",
      "Epoch [97/200], Loss: 3507.9312, Acc: 0.57%\n",
      "Epoch [98/200], Loss: 4308.5532, Acc: 0.57%\n",
      "Epoch [99/200], Loss: 6790.1465, Acc: 0.57%\n",
      "Epoch [100/200], Loss: 5226.7534, Acc: 0.57%\n",
      "Epoch [101/200], Loss: 9333.2266, Acc: 0.57%\n",
      "Epoch [102/200], Loss: 7289.9419, Acc: 0.57%\n",
      "Epoch [103/200], Loss: 7429.4214, Acc: 0.57%\n",
      "Epoch [104/200], Loss: 5468.1206, Acc: 0.57%\n",
      "Epoch [105/200], Loss: 7631.3066, Acc: 0.57%\n",
      "Epoch [106/200], Loss: 12917.5684, Acc: 0.51%\n",
      "Epoch [107/200], Loss: 8652.4482, Acc: 0.56%\n",
      "Epoch [108/200], Loss: 8182.9067, Acc: 0.54%\n",
      "Epoch [109/200], Loss: 11094.7178, Acc: 0.57%\n",
      "Epoch [110/200], Loss: 11340.7158, Acc: 0.56%\n",
      "Epoch [111/200], Loss: 14858.8809, Acc: 0.56%\n",
      "Epoch [112/200], Loss: 10947.5850, Acc: 0.56%\n",
      "Epoch [113/200], Loss: 11960.9971, Acc: 0.56%\n",
      "Epoch [114/200], Loss: 5782.5420, Acc: 0.56%\n",
      "Epoch [115/200], Loss: 6234.8887, Acc: 0.56%\n",
      "Epoch [116/200], Loss: 2305.3843, Acc: 0.56%\n",
      "Epoch [117/200], Loss: 4423.3237, Acc: 0.56%\n",
      "Epoch [118/200], Loss: 4430.9268, Acc: 0.57%\n",
      "Epoch [119/200], Loss: 2948.1516, Acc: 0.57%\n",
      "Epoch [120/200], Loss: 5088.8252, Acc: 0.57%\n",
      "Epoch [121/200], Loss: 7326.4717, Acc: 0.57%\n",
      "Epoch [122/200], Loss: 6476.5039, Acc: 0.57%\n",
      "Epoch [123/200], Loss: 4219.3047, Acc: 0.57%\n",
      "Epoch [124/200], Loss: 5156.7769, Acc: 0.57%\n",
      "Epoch [125/200], Loss: 5608.9863, Acc: 0.57%\n",
      "Epoch [126/200], Loss: 12371.7559, Acc: 0.56%\n",
      "Epoch [127/200], Loss: 13088.9248, Acc: 0.57%\n",
      "Epoch [128/200], Loss: 10034.9346, Acc: 0.57%\n",
      "Epoch [129/200], Loss: 9094.9248, Acc: 0.56%\n",
      "Epoch [130/200], Loss: 7504.9692, Acc: 0.56%\n",
      "Epoch [131/200], Loss: 14664.3848, Acc: 0.56%\n",
      "Epoch [132/200], Loss: 4848.9707, Acc: 0.56%\n",
      "Epoch [133/200], Loss: 3860.9763, Acc: 0.56%\n",
      "Epoch [134/200], Loss: 6624.9263, Acc: 0.56%\n",
      "Epoch [135/200], Loss: 4498.3701, Acc: 0.56%\n",
      "Epoch [136/200], Loss: 6442.1895, Acc: 0.57%\n",
      "Epoch [137/200], Loss: 6450.4263, Acc: 0.56%\n",
      "Epoch [138/200], Loss: 4797.4082, Acc: 0.56%\n",
      "Epoch [139/200], Loss: 9360.4297, Acc: 0.57%\n",
      "Epoch [140/200], Loss: 7754.0698, Acc: 0.58%\n",
      "Epoch [141/200], Loss: 3284.5579, Acc: 0.59%\n",
      "Epoch [142/200], Loss: 2346.3662, Acc: 0.59%\n",
      "Epoch [143/200], Loss: 3942.7234, Acc: 0.59%\n",
      "Epoch [144/200], Loss: 4105.7437, Acc: 0.59%\n",
      "Epoch [145/200], Loss: 4065.7224, Acc: 0.59%\n",
      "Epoch [146/200], Loss: 9846.8379, Acc: 0.63%\n",
      "Epoch [147/200], Loss: 10235.9238, Acc: 0.59%\n",
      "Epoch [148/200], Loss: 4973.0210, Acc: 0.58%\n",
      "Epoch [149/200], Loss: 3574.1160, Acc: 0.56%\n",
      "Epoch [150/200], Loss: 11649.0908, Acc: 0.56%\n",
      "Epoch [151/200], Loss: 6475.6494, Acc: 0.58%\n",
      "Epoch [152/200], Loss: 4241.5479, Acc: 0.59%\n",
      "Epoch [153/200], Loss: 3480.2019, Acc: 0.6%\n",
      "Epoch [154/200], Loss: 6602.2144, Acc: 0.59%\n",
      "Epoch [155/200], Loss: 6483.5332, Acc: 0.57%\n",
      "Epoch [156/200], Loss: 5425.0205, Acc: 0.56%\n",
      "Epoch [157/200], Loss: 8307.9170, Acc: 0.54%\n",
      "Epoch [158/200], Loss: 7482.9849, Acc: 0.65%\n",
      "Epoch [159/200], Loss: 9656.4238, Acc: 0.58%\n",
      "Epoch [160/200], Loss: 8428.3828, Acc: 0.58%\n",
      "Epoch [161/200], Loss: 8263.5352, Acc: 0.58%\n",
      "Epoch [162/200], Loss: 9486.6641, Acc: 0.58%\n",
      "Epoch [163/200], Loss: 7622.0269, Acc: 0.59%\n",
      "Epoch [164/200], Loss: 6141.8564, Acc: 0.58%\n",
      "Epoch [165/200], Loss: 3995.9119, Acc: 0.57%\n",
      "Epoch [166/200], Loss: 12619.8301, Acc: 0.57%\n",
      "Epoch [167/200], Loss: 5543.8545, Acc: 0.59%\n",
      "Epoch [168/200], Loss: 6273.2954, Acc: 0.59%\n",
      "Epoch [169/200], Loss: 11387.7852, Acc: 0.57%\n",
      "Epoch [170/200], Loss: 4833.0303, Acc: 0.58%\n",
      "Epoch [171/200], Loss: 4658.8589, Acc: 0.61%\n",
      "Epoch [172/200], Loss: 7041.4932, Acc: 0.63%\n",
      "Epoch [173/200], Loss: 5982.4951, Acc: 0.61%\n",
      "Epoch [174/200], Loss: 7321.6826, Acc: 0.62%\n",
      "Epoch [175/200], Loss: 13309.5361, Acc: 0.61%\n",
      "Epoch [176/200], Loss: 6284.3418, Acc: 0.62%\n",
      "Epoch [177/200], Loss: 6801.2769, Acc: 0.6%\n",
      "Epoch [178/200], Loss: 9260.8828, Acc: 0.62%\n",
      "Epoch [179/200], Loss: 5884.0049, Acc: 0.63%\n",
      "Epoch [180/200], Loss: 7228.9844, Acc: 0.62%\n",
      "Epoch [181/200], Loss: 5234.2920, Acc: 0.61%\n",
      "Epoch [182/200], Loss: 8170.6274, Acc: 0.61%\n",
      "Epoch [183/200], Loss: 5004.9360, Acc: 0.64%\n",
      "Epoch [184/200], Loss: 7151.5518, Acc: 0.61%\n",
      "Epoch [185/200], Loss: 3978.7141, Acc: 0.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [186/200], Loss: 5500.6421, Acc: 0.6%\n",
      "Epoch [187/200], Loss: 6621.1475, Acc: 0.6%\n",
      "Epoch [188/200], Loss: 4497.7412, Acc: 0.6%\n",
      "Epoch [189/200], Loss: 7575.1177, Acc: 0.6%\n",
      "Epoch [190/200], Loss: 4636.3950, Acc: 0.64%\n",
      "Epoch [191/200], Loss: 6209.1362, Acc: 0.69%\n",
      "Epoch [192/200], Loss: 5214.1055, Acc: 0.65%\n",
      "Epoch [193/200], Loss: 9021.6221, Acc: 0.62%\n",
      "Epoch [194/200], Loss: 3696.4690, Acc: 0.69%\n",
      "Epoch [195/200], Loss: 11117.6396, Acc: 0.69%\n",
      "Epoch [196/200], Loss: 5920.7061, Acc: 0.7%\n",
      "Epoch [197/200], Loss: 7826.8862, Acc: 0.73%\n",
      "Epoch [198/200], Loss: 6909.7612, Acc: 0.62%\n",
      "Epoch [199/200], Loss: 13758.5391, Acc: 0.66%\n",
      "Epoch [200/200], Loss: 12342.5947, Acc: 0.65%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(data)):  \n",
    "        output = model.forward(torch.tensor([i]))\n",
    "        if random.random() < .9:\n",
    "            chosen_action = torch.distributions.Categorical(fn.softmax(output,1)).sample()\n",
    "        else:\n",
    "            #chosen_action = torch.argmax(output)\n",
    "            chosen_action = torch.tensor([random.randint(0, 5)]).type(torch.LongTensor)\n",
    "\n",
    "        #chosen_action = torch.tensor([random.randint(0, 2)])\n",
    "        #pdb.set_trace()\n",
    "        responsible_weight = output.gather(1, chosen_action.view(-1, 1))\n",
    "        mse_loss = loss(responsible_weight.view(-1), sample(data[i])[chosen_action]) + 0.7 * loss(model.embed(torch.tensor([i])), torch.zeros_like(model.embed(torch.tensor([i]))))\n",
    "        total_loss += mse_loss\n",
    "        optimizer.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1== 0:\n",
    "        embd = model.embed.weight.mean(0).repeat(1, 100).view(100, -1)\n",
    "        #model.embed.weight = nn.Parameter(embd)\n",
    "        correct = 0\n",
    "        for i in range(len(data)):\n",
    "            if torch.argmax(model.forward(torch.tensor([i]))) == torch.argmax(data[i]):\n",
    "                correct += 1\n",
    "#             if torch.argmax(model.forward(torch.tensor([i]))) != torch.argmax(data[i]):\n",
    "#                 print data[i]\n",
    "#                 print model.forward(torch.tensor([i]))\n",
    "#                 print \"----------\"\n",
    "#         pdb.set_trace()\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}, Acc: {}%' \n",
    "                       .format(epoch+1, num_epochs,  total_loss / len(data), float(correct)/len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 897.3497,  246.2585,  832.8798,  780.5245,  184.0292,  516.8893]) tensor(0)\n",
      "tensor([[ 745.3607,  504.9086,  875.2828,  347.3766,  398.0196,  492.4910]]) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(data))\n",
    "print (data[i]), torch.argmax(data[i])\n",
    "print (model.forward(torch.tensor([i]))), torch.argmax(model.forward(torch.tensor([i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embd = model.embed.weight.mean(0).repeat(1, 100).view(100, -1)\n",
    "model.embed.weight = nn.Parameter(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fc1.requires_grad = False\n",
    "model.fc2.requires_grad = False\n",
    "model.fc3.requires_grad = False\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)  \n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3888.9941, Acc: 0.46%\n",
      "Epoch [2/10], Loss: 6521.1162, Acc: 0.46%\n",
      "Epoch [3/10], Loss: 14658.1953, Acc: 0.47%\n",
      "Epoch [4/10], Loss: 3618.3181, Acc: 0.47%\n",
      "Epoch [5/10], Loss: 3006.0510, Acc: 0.47%\n",
      "Epoch [6/10], Loss: 2979.8181, Acc: 0.47%\n",
      "Epoch [7/10], Loss: 2513.6782, Acc: 0.44%\n",
      "Epoch [8/10], Loss: 13323.3047, Acc: 0.45%\n",
      "Epoch [9/10], Loss: 5533.4487, Acc: 0.47%\n",
      "Epoch [10/10], Loss: 4660.0049, Acc: 0.47%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(data)):  \n",
    "        output = model.forward(torch.tensor([i]))\n",
    "        chosen_action = torch.distributions.Categorical(fn.softmax(output,1)).sample()\n",
    "        #chosen_action = torch.argmax(output)\n",
    "\n",
    "        #chosen_action = torch.tensor([random.randint(0, 2)])\n",
    "        #pdb.set_trace()\n",
    "        responsible_weight = output.gather(1, chosen_action.view(-1, 1))\n",
    "        mse_loss = loss(responsible_weight.view(-1), sample(data[i])[chosen_action])\n",
    "        total_loss += mse_loss\n",
    "        optimizer.zero_grad()\n",
    "        mse_loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 1 == 0:\n",
    "        correct = 0\n",
    "        for i in range(len(data)):\n",
    "            if torch.argmax(model.forward(torch.tensor([i]))) == torch.argmax(data[i]):\n",
    "                correct += 1\n",
    "#             if torch.argmax(model.forward(torch.tensor([i]))) != torch.argmax(data[i]):\n",
    "#                 print data[i]\n",
    "#                 print model.forward(torch.tensor([i]))\n",
    "#                 print \"----------\"\n",
    "#         pdb.set_trace()\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}, Acc: {}%' \n",
    "                       .format(epoch+1, num_epochs,  total_loss / len(data), float(correct)/len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  95.6899,  376.7789,  604.5927,  899.2483,  263.2460,  758.4307]) tensor(3)\n",
      "tensor([[  93.0321,  142.1794,   78.7361,  -19.5789,  170.4666,  105.8602]]) tensor(4)\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(data))\n",
    "print (data[i]), torch.argmax(data[i])\n",
    "print (model.forward(torch.tensor([i]))), torch.argmax(model.forward(torch.tensor([i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
